# BEVFormer 系列论文总结



## BEVFormer

![bevformer-structure](../graph/image-20251202211237022.png)

### 论文动机

1. 3D目标检测任务需要强的BEV特征去支撑准确的3D bbox 预测，但是从2D平面生成3D特征是病态的。当前框架主要利用深度信息生成BEV特征，对深度的准确性和分布敏感，而不准确的BEV特征会严重损害模型性能。**论文设计了BEV生成方式，不依赖于深度信息，并切能够自适应地学习BEV特征，而不是严格地依赖于3D 先验。使用Transformer 注意力机制去动态地聚合有价值的特征，获得更好的BEV特征**
2. 利用BEV特征去执行感知任务，是由于**BEV特征是连接空间（spatial）和时间（temporal）的可用的桥梁。**时序信息在感知中扮演着重要的角色，如推理目标的运动状态和识别遮挡物体。**我们循环（recurrently）地利用BEV特征从过去到现在传递时间信息，这与RNN模型的隐藏状态具有相同的本质。**
3. 论文基于**Deformable Attention**模型实现了一种融合多视角相机和时序特征的端到端框架，适用于多种感知任务。

### 关键点

#### BEV Query

BEV查询是**BEV网格(grid)形状**的**可学习参数**，*用于查询BEV特征图*，从跨摄像头中查找空间特征，并从历史BEV特征中查找时序特征。

![image-20251210160926809](../graph/image-20251210160926809.png)

* 每个query为形状为 $H\times W \times C$ 的可学习参数，用来获取BEV特征，其中$H \times W$表示BEV平面的空间形状。
* 每个位于$(x,y)$网格位置的query仅负责查询，表征其对应的小范围区域，每个查询的网格单元对应$s$米的真实大小。
* 通过轮番/迭代查询 空间和时序信息，生成BEV特征。
* 根据普遍的经验，在将查询$Q$输入tansformer之前，添加一个可学习的位置编码（position-embedding）到BEV查询中$Q$中。

![image-20251210162406704](../graph/image-20251210162406704.png)



#### Spatial Cross-Attention(SCA)

空间交叉注意力模块，*用于融合多视角特征*，去集成来自多个相机图片的空间特征。

![spatial-query](../graph/image-20251210161755631.png)

具体步骤如下

1. 第一步，将$(x, y)$位置的BEV 查询$Q$，通过采样不同的高度，将查询点提升(lift)为柱体(pillar)的不同高度的3D点。
2. 第二步，从柱体采样采样$N_{ref}$个3D点，并利用相机内外参，将采样的3D点投影到不同相机视角下的2D平面。
3. 第三步，**将2D平面击中的的点视为查询$Q$的参考点**，从被击中(hit)视角的对应区域，通过Deformable Attention方式采样特征。
4. 第四步，使用加权方式融合从击中视角采样到的特征，得到$(x,y)$查询点对应的BEV特征。

#### Temporal Self-Attention(TSA)

时序自注意力模块，*用于融合时序BEV特征*，从历史BEV特征中提取时序信息。

前后帧的时序信息对于推理运动目标的速度和预测遮挡率高的目标至关重要。

![temporal-query](../graph/image-20251210161927794.png)



具体步骤如下

1. 第一步，根据自车运动信息（定位和姿态），将两帧特征对齐到相同坐标系（align）,使得同一查询网格上的特征对应同一实际位置。
2. 第二步，使用self-attention，分别从对齐后的当前帧特征和历史帧特征去采样特征。
3. 第三步，使用加权方式，融合采样到的当前帧BEV特征和历史帧BEV特征。
4. 第四步，采用RNN风格的方式，迭代地收集历史帧的BEV特征。

### 代码解析



## BEVFormerV2





## 参考资料

* https://zhuanlan.zhihu.com/p/543335939
* 