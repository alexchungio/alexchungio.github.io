# LSS 论文总结

## 背景

在自动驾驶或者机器人领域，需要从摄像头获取的二维图像中提取三维信息，特别是从任意配置的摄像头中获取三维信息。从二维图像中的点映射回3D空间中，即反投影过程（back-projection）,其核心是通过相机内参和外参建立几何映射关系。

反投影得到的是从相机光心出发经过图像点的射线，*而非唯一的3D坐标*。需要依赖深度值z，去计算具体的3D位置坐标。

> 视觉BEV感知的核心在于将透视视图下的图像信息变换到BEV视图。这是一个理论上的病态问题，因为一个图像像素会对应3D空间中的多个位置。这种一对多的歧义性是因为我们不知道像素在3D空间中对应的高度或者深度。这二者知其一（下图中的Oy或者Oz），我们就可以通过相机的内外参矩阵将图像的每一个像素都变换到3D空间。
>
> 因此，**图像到BEV的视图转换也大致有两种思路：一种假设高度（Oy）已知，一种假设深度（Oz）已知**。
>
> 前者的典型思路是假设目标位于路面，因此高度为0，这样就可以采用Homography变换将图像转换到BEV视图，比如**BEV-IPM**。在实际应用中，路面并非绝对平坦，而且除了车道线以外，其他的目标都有一定的高度。在这些情况下，高度已知的假设并不成立，变换后的图像误差也很大。因此，反变换的方法在BEV感知中采用的相对较少。
>
> 后者通常会依赖于主动传感器直接获得深度信息（比如激光雷达）或者从图像中通过显式或者隐式的方式获得深度信息。

### 概述

LSS(Lift-Splat-Shoot) 范式的本质是三步：第一步，通过2D特征和每个像素的深度估计，形成2.5D的**相机（camera）坐标系**的视锥点云/视锥特征；第二步，通过相机内外参，将视锥点云从相机坐标系转换到**自车（ego）坐标系**，形成3D空间的**稠密点云**；第三步，构建**体素空间**（点云空间的稀疏化表示），通过`voxel_pooling`对同一网格的所有点云进行求和池化，形成BEV特征。

具体论文中， Lift 对应第一步，Splat 包含第二步和第三步， 而Shoot，表示在BEV特征图上执行下游任务。

## 关键点

### Lift(提升，估计潜在深度分布)

Lift的作用是将单目（monocular）图像的2D特征提升到3D空间，生成视锥点云（Frustum Point Cloud）

融合多个单目相机的难度在于，需要依赖深度信息才能将像素投影到参考坐标系（自车/世界坐标系），但是对于二维图像每个像素的“深度”本质上是模糊的。

#### 执行步骤

* 生成视锥坐标云

  Lift，为每个像素添加depth信息时，通过为**每个像素估计一个深度分布（每个像素在不同深度的概率）**，*通过离散深度采样代替显示深度预测，避免误差累计*，将2D的图像的点“提升”到假设的3D空间中的点云。生成的3D点云包含所有可能的三维坐标分布，三维坐标的深度分布通过BEV空间的具体的监督任务进行学习。

  第一步，根据图像特征大小和深度大小生成**基于图像坐标系**的视锥点frustum_point: (B,D,H,W,3)，

  第二步，利用相机内外参，将视锥点由图像坐标系转换到自车（ego）坐标系，得到最终的视锥体点云frustum_point: (B,D,H,W,3)。 

* 构建视锥/图像特征点

  通过特征外积构建视锥特征，对于每个像素的深度特征（depth_feature: (B, D, H, W)）和语义特征（img_feature: (B, C, H, W)）通过**外积**操作， 生成每个深度对应的3D特征，构造视锥特征(frustum_feature: (B, C, D, H, W))。

​		第一步，输入图像分别提取特征图每个位置对应的深度方向概率分布特征和语义特征 (x_feature: (B, D+C, H, W), D表示离散深度的维度，C表示每个位置语义特征的维度)。

​		第二步，对估计出来的离散深度特征(B, D, H, W)，执行softmax操作，计算深度方向的概率密度，得到深度概率密度特征（B, D, H, W）。

​		第三步，利用深度概率密度特征(B, D, H, W)和语义特征(B, C, H, W)，执行外积操作，构建视锥特征点（B, D, H, W, C）

### Splat(拍平/溅射/Pillar Pooling)

Splat，将多个相机通过Lift中“提升”得到的3D点云投影到统一的BEV空间，生成BEV特征（俯视图特征）。

Pillar Pooing 中的“柱”是有无限高度的体素网格，分配每个点云到最近的“柱”，然后执行加和池化去构建BEV特征（bev_feature: (B,C,H,W)）。

#### 执行步骤

* 坐标系变换

  将图像坐标系的点云转换到自车坐标系

* 融合不同摄像头点云

  融合不同摄像头的点云信息，生成一个稠密的三维表示

* 体素池化（voxel pooling）
  * 构建固定大小的体素/网格空间，为每个点分配网格索引
  * 累计池化，通过网格索引聚合特征。

### Shoot(预测)

Shoot，在BEV特征图上执行下游任务，如OCC、3D-OD或轨迹预测等。

#### 执行步骤

* BEV特征编码
* 任务头设计

## 参考资料

* [LSS论文与代码详解-掘金](https://juejin.cn/post/7385375139305668627)
* https://zhuanlan.zhihu.com/p/567926611
* https://zhuanlan.zhihu.com/p/668846159
* https://zhuanlan.zhihu.com/p/6144337348

* https://jordicenzano.name/front-test/2d-3d-paradigm-overview-2011/camera-model/
* https://zhuanlan.zhihu.com/p/654311603?utm_id=0
* https://developer.d-robotics.cc/forumDetail/143772473308124163

