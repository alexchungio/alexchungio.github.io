# LSS 论文总结

## 背景

在自动驾驶或者机器人领域，需要从摄像头获取的二维图像中提取三维信息，特别是从任意配置的摄像头中获取三维信息。从二维图像中的点映射回3D空间中，即反投影过程（back-projection）,其核心是通过相机内参和外参建立几何映射关系。

反投影得到的是从相机光心出发经过图像点的射线，*而非唯一的3D坐标*。需要依赖深度值z，去计算具体的3D位置坐标。

### 概述

LSS(Lift-Splat-Shoot) 范式的本质是三步：第一步，通过2D特征和每个像素的深度估计，形成2.5D的**相机（camera）坐标系**的视锥点云/视锥特征；第二步，通过相机内外参，将视锥点云从相机坐标系转换到**自车（ego）坐标系**，形成3D空间的**稠密点云**；第三步，构建**体素空间**（点云空间的稀疏化表示），通过`voxel_pooling`对同一网格的所有点云进行求和池化，形成BEV特征。

具体论文中， Lift 对应第一步，Splat 包含第二步和第三步， 而Shoot，表示在BEV特征图上执行下游任务。

## 关键点

### Lift(提升，估计潜在深度分布)

Lift的作用是将单目（monocular）图像的2D特征提升到3D空间，生成视锥点云（Frustum Point Cloud）

融合多个单目相机的难度在于，需要依赖深度信息才能将像素投影到参考坐标系（自车/世界坐标系），但是对于二维图像每个像素的“深度”本质上是模糊的。

#### 执行步骤

* 生成视锥点云

  Lift，为每个像素添加depth信息时，通过为**每个像素估计一个深度分布（每个像素在不同深度的概率）**，*通过离散深度采样代替显示深度预测，避免误差累计*，将2D的图像的点“提升”到假设的3D空间中的点云。生成的3D点云包含所有可能的三维坐标分布，三维坐标的深度分布通过BEV空间的具体的监督任务进行学习。

  根据图像特征大小和深度大小生成视锥点frustum_point: (B,D,H,W,3)

* 特征外积

  通过特征外积构建视锥特征，对于每个像素的深度特征（depth_feature: (B, D, H, W)）和语义特征（img_feature: (B, C, H, W)）通过**外积**操作， 生成每个深度对应的3D特征，构造视锥特征(frustum_feature: (B, C, D, H, W))。

​		略

### Splat(拍平/溅射/Pillar Pooling)

Splat，将多个相机通过Lift中“提升”得到的3D点云投影到统一的BEV空间，生成BEV特征（俯视图特征）。

Pillar Pooing 中的“柱”是有无限高度的体素网格，分配每个点云到最近的“柱”，然后执行加和池化去构建BEV特征（bev_feature: (B,C,H,W)）。

#### 执行步骤

* 坐标系变换

  将图像坐标系的点云转换到自车坐标系

* 融合不同摄像头点云

  融合不同摄像头的点云信息，生成一个稠密的三维表示

* 体素池化（voxel pooling）
  * 构建固定大小的体素/网格空间，为每个点分配王哥索引
  * 累计池化，通过网格索引聚合特征。

### Shoot(预测)

Shoot，在BEV特征图上执行下游任务，如OCC、3D-OD或轨迹预测等。

#### 执行步骤

* BEV特征编码
* 任务头设计

## 参考资料

* [LSS论文与代码详解-掘金](https://juejin.cn/post/7385375139305668627)
* https://zhuanlan.zhihu.com/p/567926611
* https://zhuanlan.zhihu.com/p/668846159
* https://zhuanlan.zhihu.com/p/6144337348

* https://jordicenzano.name/front-test/2d-3d-paradigm-overview-2011/camera-model/
* https://zhuanlan.zhihu.com/p/654311603?utm_id=0

