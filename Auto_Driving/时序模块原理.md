# 时序模块原理

## BEVDet4D

### 关键代码

* BEV 特征提取

  ```python
  # projects/mmdet3d_plugin/models/detectors/bevdet4d.py
  def prepare_bev_feat(self, img, sensor2egos, ego2globals, intrin, post_rot, post_tran,
                       bda, mlp_input):
      """
      Args:
          imgs:  (B, N_views, 3, H, W)
          sensor2egos: (B, N_views, 4, 4)
          ego2globals: (B, N_views, 4, 4)
          intrins:     (B, N_views, 3, 3)
          post_rots:   (B, N_views, 3, 3)
          post_trans:  (B, N_views, 3)
          bda_rot:  (B, 3, 3)
          mlp_input:
      Returns:
          bev_feat: (B, C, Dy, Dx)
          depth: (B*N, D, fH, fW)
      """
      x, _ = self.image_encoder(img)      # x: (B, N, C, fH, fW)
      
      # extract bev_feat
      # bev_feat: (B, C * Dz(=1), Dy, Dx)
      # depth: (B * N, D, fH, fW)
      bev_feat, depth = self.img_view_transformer(
          [x, sensor2egos, ego2globals, intrin, post_rot, post_tran, bda, mlp_input])
  		
      # encoder after extract bev_feat
      # w.r.t paper: 4.3.3 The Position of the Temporal Fusion
      if self.pre_process:
          bev_feat = self.pre_process_net(bev_feat)[0]    # (B, C, Dy, Dx)
      return bev_feat, depth
  ```

* 时序特征融合

  ```python
  def extract_img_feat_sequential(self, inputs, feat_prev):
      """
      Args:
          inputs:
              curr_img: (1, N_views, 3, H, W)
              sensor2keyegos_curr:  (N_prev, N_views, 4, 4)
              ego2globals_curr:  (N_prev, N_views, 4, 4)
              intrins:  (1, N_views, 3, 3)
              sensor2keyegos_prev:  (N_prev, N_views, 4, 4)
              ego2globals_prev:  (N_prev, N_views, 4, 4)
              post_rots:  (1, N_views, 3, 3)
              post_trans: (1, N_views, 3, )
              bda_curr:  (N_prev, 3, 3)
          feat_prev: (N_prev, C, Dy, Dx)
      Returns:
  
      """
      imgs, sensor2keyegos_curr, ego2globals_curr, intrins = inputs[:4]
      sensor2keyegos_prev, _, post_rots, post_trans, bda = inputs[4:]
      bev_feat_list = []
      mlp_input = self.img_view_transformer.get_mlp_input(
          sensor2keyegos_curr[0:1, ...], ego2globals_curr[0:1, ...],
          intrins, post_rots, post_trans, bda[0:1, ...])
      inputs_curr = (imgs, sensor2keyegos_curr[0:1, ...],
                     ego2globals_curr[0:1, ...], intrins, post_rots,
                     post_trans, bda[0:1, ...], mlp_input)
  		
      
      # ############## extract bev_feat ###################
      # (1, C, Dx, Dy), (1*N, D, fH, fW)
      bev_feat, depth = self.prepare_bev_feat(*inputs_curr)
      bev_feat_list.append(bev_feat)
  
      # ###########align the feat_prev to currrent ego-coord  ############
      _, C, H, W = feat_prev.shape
      # feat_prev: (N_prev, C, Dy, Dx)
      feat_prev = \
          self.shift_feature(feat_prev,   # (N_prev, C, Dy, Dx)
                             [sensor2keyegos_curr,    # (N_prev, N_views, 4, 4)
                              sensor2keyegos_prev],   # (N_prev, N_views, 4, 4)
                              bda  # (N_prev, 3, 3)
                             )
      bev_feat_list.append(feat_prev.view(1, (self.num_frame - 1) * C, H, W))     # (1, N_prev*C, Dy, Dx)
  		
      # fusion feat_curr and feat_prev
      bev_feat = torch.cat(bev_feat_list, dim=1)      # (1, N_frames*C, Dy, Dx)
      x = self.bev_encoder(bev_feat)
      return [x], depth
  ```



## gridsample 方式实现

### 方案简述

保存多帧历史帧数据，并将历史帧数据与当前帧数据融合，构建包含历史信息的融合帧。

历史帧特征到当前帧特征到映射包含两步：

*  通过历史帧和当前帧的全局位姿信息，计算当前帧BEV特征点对应历史BEV特征的点坐标，构建lut表
* 利用gridsample 直接索引历史帧特征到当前帧。

### 坐标转换

### BEV坐标系到ego 坐标系转换关系（T1）

假设特征BEV坐标为$(u,v)$，ego坐标系的坐标为 $(x,y,z)$

$$\left \{ \begin{array}{l}  x=(u/bev_w)\ast{bev\_range_w}+T_x \\ y=(v/bev_h)\ast{bev\_range_h}+T_y \\ z=0 \end{array} \right.$$

其中 $bev_w$ 和 $bev_h$ 分别代表BEV特征图的宽和高； $bev\_range_w$和$bev\_range_h$分别表示BEV特征对应物理世界坐标的宽和高，单位为m; $T_x$和$T_y$表示自车坐标系坐标原点的偏移量，一般使用自车后轴中心作为坐标。

### ego 坐标系到ego 坐标系的转换关系（T2）

假设自车自车坐标系$P_t$的齐次次坐标为$(x,y,z,1)$，从$t$帧自车坐标系转换到$t+1$帧的自车坐标系转换公式如下，

$$P_{t+1}=Pose_{t+1}^{-1} \cdot Pose_{t} \cdot P_t$$

上述公式可以分为两步，第一步，通过$t$时刻的姿态矩阵，将自车坐标系转换到全局坐标系的坐标$P_g=Pose_t \cdot P_t$；第二步，利用全局坐标$P_t$和$t+1$时刻的姿态矩阵的逆矩阵，得到$t$时刻自车坐标到$t+1$时刻自车系坐标

### BEV坐标系到BEV坐标系的转换

获取当前帧BEV坐标的点对应历史帧BEV特征点的坐标，转换公式矩阵如下

$T = (T_1')^{-1} \cdot T_2 \cdot T_1$

通过获得的转换矩阵，构建历史帧BEV特征到当前BEV特征的lut表，利用获得的lut表，使用grid_sample 索引历史帧BEV特征到当前帧对应的BEV特征

### 备注

构建索引表的时候，**需要需要结合grid_sample 的实现方式，执行不同的offset操作，去适配索引坐标**。

### 关键代码



